"""
University Entrance Exam as a Guiding Test for Artificial Intelligence
https://www.ime.usp.br/~ddm/project/enem/ENEM-GuidingTest.pdf

The ENEM Challenge consists in designing an autonomous system that matches the 
performance of a human students on the exam. The overall goal is to foster and 
evaluate the development of Artificial Intelligence techniques that have good 
performance on complex cognitive tasks, not particularly designed for AI systems. 
In addition, this challenge aims to promote and give more visiblity to the 
development of NLP tools for Brazilian Portuguese.

Homepage: https://www.ime.usp.br/~ddm/project/enem
"""
import collections
from io import BytesIO
import json
import numpy as np
import os
import re
from urllib.request import urlopen
import xml.etree.ElementTree as ET 
from zipfile import ZipFile
from datasets import load_dataset
from lm_eval import utils
from lm_eval.base import Task, rf
from lm_eval.metrics import mean
from fastchat.conversation import get_conv_template, register_conv_template, Conversation
import re


_CITATION = """
@InProceedings{ ENEM-Challenge,
    author={Silveira, Igor Cataneo and Mau\'a, Denis Deratani},
    booktitle={Proceedings of the 6th Brazilian Conference on Intelligent Systems},
    series={BRACIS},
    title={University Entrance Exam as a Guiding Test for Artificial Intelligence},
    pages={426--431},
    year={2017}
}
"""


PATTERNS_REPLACES = [
    (r'\s*\n+\s*', r' '),  # changing \n to space
    (r'(\s)\1+', r' '),  # changing \n to space
    (r'^\s+', r''),
]


apply_regex = lambda pattern, replace, text: re.sub(pattern, replace, text)
# blind 

from enum import Enum
# enumerator with blind, image and caption 
class BLUEX_EVAL_MODE:
    BLIND = 0
    IMAGE = 1
    CAPTION = 2
    CONTEXT_CAPTION = 3



class MEMO_TRAP_PT(Task):
    VERSION = 0
    DATASET_PATH = 'portuguese-benchmark-datasets/proverbs'
    DATASET_NAME = 'proverbs_ptbr'
    COT = False
    mode= BLUEX_EVAL_MODE.BLIND



    use_just_linguistic_and_humanities = False
    tag = None

    def download(self, data_dir=None, cache_dir=None, download_mode=None):
        # download and process dataset
        dataset = load_dataset(self.DATASET_PATH, self.DATASET_NAME,  download_mode='force_redownload', token="")["train"]

        self.dataset = collections.defaultdict(list)

        for example in dataset:
            self.dataset["test"].append(example)

        self.dataset["test"] = list(map(self._process_doc, self.dataset["test"]))

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True
        
    def test_docs(self):
        return self.dataset["test"]

    def _process_doc(self, doc):
        return {
            "query": doc["prompt"], #O que vai pro modelo
            "choices": doc["classes"], #Suas escolhas
            "gold": doc["answer"], #resposta
            "id": doc["id"], #id
            "proverb": doc["proverb"] #proverb
        }

    def doc_to_text(self, doc):
        return doc["query"]

    def doc_to_target(self, doc): #Recebe dicionário e tem que retornar qual a resposta correta. 
        return doc["gold"]

    def construct_requests(self, doc, ctx):
        """ Uses RequestFactory to construct Requests and returns an iterable of 
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural 
            language description, as well as the few shot examples, and the question
            part of the document for `doc`. 
        """
        continuation = rf.greedy_until(ctx, []) # rf -> Request factory
        return continuation

    def process_results(self, doc, results):
        gold = doc["gold"]
        pred = results[0]
        print("pred:", pred)
        print("gold:", gold)
        
        is_gold_in_pred = gold in pred
        print("Gold is in Pred: ", is_gold_in_pred)
        
        # Acurácia será 1.0 se 'gold' estiver em 'pred', caso contrário será 0.
        acc = 1.0 if is_gold_in_pred else 0.

        results = {
            "acc": acc,
        }

        return results
    
    def higher_is_better(self):
        return {
            "acc": True,
        }

    def aggregation(self):
        def safe_mean(values):
            if len(values) == 0:
                return -1
            return mean(values)
        
        return {
            "acc": safe_mean,
        }
    
    @utils.positional_deprecated
    def fewshot_context(self, doc, num_fewshot, provide_description=None, rnd=None, description=None, conversation_template=None, prompt_as_single_user_message=False):
        """ Returns a fewshot context string that is made up of a prepended description
        (if provided), the `num_fewshot` number of examples, and an appended prompt example.

        :param doc: str
            The document as returned from training_docs, validation_docs, or test_docs.
        :param num_fewshot: int
            The number of fewshot examples to provide in the returned context string.
        :param prompt_mode: str
            The type of prompt. Please set prompt_mode as "fixed", "dynamic-random", or "dynamic-similar".
            WARNING: this is implemented only for Portuguese tasks.
        :param provide_description: bool
            Not implemented, and this option is deprecated and will be removed in a future version in favor of a different description providing method
        :param rnd: random.Random
            The pseudo-random number generator used to randomly sample examples.
            WARNING: This is currently a required arg although it's optionalized with a default `None`.
        :param description: str
            The task's description that will be prepended to the fewshot examples.
        :returns: str
            The fewshot context.
        """
        assert rnd is not None, "A `random.Random` generator argument must be provided to `rnd`"
        assert not provide_description, (
            "The `provide_description` arg will be removed in future versions. To prepend "
            "a custom description to the context, supply the corresponding string via the "
            "`description` arg."
        )
        if provide_description is not None:
            # nudge people to not specify it at all
            print("WARNING: provide_description is deprecated and will be removed in a future version in favor of description_dict")
        
        register_conv_template(
            Conversation(
                name="null_template",  # Certifique-se de usar o nome que você deseja
                roles=("user", "assistant"),
                sep_style=None,
                sep=None,
                system_message=None
            ), override=True
        )

        if conversation_template:
            conversation = get_conv_template("null_template")  # Use o novo template registrado
            user_role, assistant_role = conversation.roles
            assert description, "Conversation prompt requires a description."

        else:
            description = description + "\n\n" if description else ""

        # if conversation_template:
        #     conversation = get_conv_template(conversation_template)
        #     user_role, assistant_role = conversation.roles
        #     assert description, "Conversation prompt requires a description."
        # else:
        #     description = description + "\n\n" if description else ""

        example = self.doc_to_text(doc)

        if num_fewshot == 0:
            labeled_examples = ""
            if conversation_template:
                conversation.append_message(user_role, description + "\n" + example)
                conversation.append_message(assistant_role, None)
        else:
            # for sets with no training docs, draw from other set *but ensure no overlap with current doc*
            if self.has_training_docs():
                # fewshotex = self.fewshot_examples(k=num_fewshot, rnd=rnd)
                ## keeping the training docs in original order (use this to fixed prompts)
                fewshotex = list(self.training_docs())[:num_fewshot]
                ## if the current doc is among the training docs, we do not use it as few-shot
                fewshotex = [ex for ex in fewshotex if doc['id'] != ex['id']]
            else:
                if self._fewshot_docs is None:
                    self._fewshot_docs = list(
                        self.validation_docs() if self.has_validation_docs() else self.test_docs()
                    )

                fewshotex = rnd.sample(self._fewshot_docs, num_fewshot + 1)

                # get rid of the doc that's the one we're evaluating, if it's in the fewshot
                fewshotex = [x for x in fewshotex if x != doc][:num_fewshot]

            labeled_examples = ''
            
            if conversation_template:
                conversation.append_message(user_role, description)
                conversation.append_message(assistant_role, "Ok.")

                for i, doc_ex in enumerate(fewshotex):
                    text = self.doc_to_text(doc_ex)
                    target = self.doc_to_target(doc_ex).strip()
                    conversation.append_message(user_role, text)
                    conversation.append_message(assistant_role, target)
                
                conversation.append_message(user_role, example)
                conversation.append_message(assistant_role, None)
            else: #Corrigir para fazer um exemplo manual
                for i, doc_ex in enumerate(fewshotex):
                    labeled_examples += f'Questão {i+1}:\n'
                    labeled_examples += self.doc_to_text(doc_ex) + self.doc_to_target(doc_ex)
                    labeled_examples += '\n##\n'
                labeled_examples += f'Questão {len(fewshotex) + 1}:\n'
            print("CONVERSATION:", conversation)
        if conversation_template:
            if prompt_as_single_user_message:
                return conversation.get_prompt()
            else:
                messages= conversation.to_openai_api_messages()

                # remove any system messages
                messages = [m for m in messages if m["role"] != "system"]
                
                return json.dumps(messages, ensure_ascii=False)
        else:
            return description + labeled_examples + example